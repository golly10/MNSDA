{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MetodosNoSupervisados-EjemploDensidadProbabilidad.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFf-0Eb6D58h"
      },
      "source": [
        "# Practica con métodos de agrupamiento basados en densidad y en probabilidad con este notebook en Python\n",
        "En este *notebook* aprenderás a ejecutar un método de agrupamiento basado en densidad (DBSCAN) y otro basado en probabilidad (GMM/Expectation-Maximization) utilizando las librerías *PyClustering* y *scikit-learn*. Veremos cómo configurar sus parámetros y visualizar sus resultados."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Ejecutar el algoritmo DBSCAN en ***PyClustering***\n",
        "En la primera parte del *notebook* utilizaremos la librería *PyClustering*. En concreto, nos centraremos en el paquete [*cluster.dbscan*](https://pyclustering.github.io/docs/0.8.2/html/d2/d42/classpyclustering_1_1cluster_1_1dbscan_1_1dbscan.html), donde se encuentra la implementación del método DBSCAN. Como en notebooks anteriores, primero tenemos que importar los paquetes que necesitaremos:"
      ],
      "metadata": {
        "id": "e_gVu9_ReO6o"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLVTAhVEDpeY"
      },
      "source": [
        "# La primera vez que se vaya a ejecutar este notebook es necesario instalar la librería pyclustering\n",
        "!pip install pyclustering\n",
        "from pyclustering.cluster.dbscan import dbscan\n",
        "from pyclustering.cluster import cluster_visualizer\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wi-3ZOlEFUza"
      },
      "source": [
        "Vamos a utilizar el *dataset* Iris para este ejemplo, ya que viene incorporado en la librería. De sus cuatro propiedades numéricas, nos quedaremos con las dos primeras a las que llamaremos *x* e *y*. Representamos esta muestra bi-dimensional con *matplotlib* para ver la distribución de los datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvAYZpjnGUtU"
      },
      "source": [
        "from pyclustering.utils import read_sample\n",
        "from pyclustering.samples.definitions import FAMOUS_SAMPLES\n",
        "datos = read_sample(FAMOUS_SAMPLES.SAMPLE_IRIS)\n",
        "x = [punto[0] for punto in datos]\n",
        "y = [punto[1] for punto in datos]\n",
        "datos_xy = np.column_stack((x, y))\n",
        "print(x)\n",
        "print(y)\n",
        "plt.scatter(x, y)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIXHpKX8HRrJ"
      },
      "source": [
        "A continuación, podemos configurar los parámetros del algoritmo DBSCAN. Tenemos que especificar el valor de *eps* (radio de vecindad) y el de *neighbors* (número mínimo de vecinos)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWecB35NHriH"
      },
      "source": [
        "eps = 0.25\n",
        "neighbors = 5\n",
        "alg_dbscan = dbscan(datos_xy, eps, neighbors);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "355eOJEtIMNd"
      },
      "source": [
        "Para ejecutar el análisis de grupos, invocamos a la función *process* como hacíamos con otros métodos de esta misma librería."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8v9gR9BEJK4k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d51cd75d-356f-4524-dd2c-230b7d53a2e2"
      },
      "source": [
        "alg_dbscan.process()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyclustering.cluster.dbscan.dbscan at 0x7f29d45070d0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gL0J841-JadE"
      },
      "source": [
        "Una vez ejecutado el algoritmo, podemos obtener los grupos por medio del método *get_clusters*. En este caso, no sabemos de antemano cuántos grupos se han obtenido, pero podemos averiguarlo con el método *len*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayqFxZAfJh8x"
      },
      "source": [
        "# Devuelve un array de k elementos, donde cada elemento continene el índice de las instancias asignadas al grupo k\n",
        "grupos = alg_dbscan.get_clusters()\n",
        "print(grupos)\n",
        "num_grupos = len(grupos)\n",
        "print(num_grupos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Además, el algoritmo DBSCAN aisla los puntos considerados como \"ruido\" o \"outliers\". Para obtenerlos, utilizamos el método *get_noise*."
      ],
      "metadata": {
        "id": "NFzIGJyjfoy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ruido = alg_dbscan.get_noise()\n",
        "print(ruido)\n",
        "num_outliers = len(ruido)\n",
        "print(num_outliers)"
      ],
      "metadata": {
        "id": "lLwazIrQf4SD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqwBUAvmJr3j"
      },
      "source": [
        "Podemos visualizar la asignación en una gráfica bidimensional utilizando la clase *visualizer*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFIh3cVNDHHI"
      },
      "source": [
        "grafico = cluster_visualizer();\n",
        "grafico.append_clusters(grupos, datos_xy)\n",
        "grafico.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M78EsmVuDGyw"
      },
      "source": [
        "## 2. Ejecutar el algoritmo Expectation-Maximization en ***PyClustering***\n",
        "Vamos ahora a trabjar con el agrupamiento basado en probabilidad que nos ofrece *PyClustering*. Se trata del modelo de mezcla de distribuciones gaussianas con el algoritmo *Expectation-Maximization*. Este algoritmo está en el paquete [*cluster.ema*](https://pyclustering.github.io/docs/0.8.2/html/d1/d24/namespacepyclustering_1_1cluster_1_1ema.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55jToREKFQF3"
      },
      "source": [
        "from pyclustering.cluster.ema import ema, ema_initializer, ema_observer, ema_visualizer, ema_init_type"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Según su [documentación](https://pyclustering.github.io/docs/0.8.2/html/d4/d22/classpyclustering_1_1cluster_1_1ema_1_1ema__init__type.html), podemos seleccionar el tipo de inicialización entre aleatoria o basada en *k-means++*. En este caso, vamos a utilizar la forma de inicialización no aleatoria, por lo que primero necesitamos estimar los valores iniciales de medias y la matriz de covarianzas. "
      ],
      "metadata": {
        "id": "lnEElcnzhhOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_grupos = 3\n",
        "medias_inicial, covarianzas_inicial = ema_initializer(datos_xy, num_grupos).initialize(ema_init_type.KMEANS_INITIALIZATION)\n",
        "print(medias_inicial)\n",
        "print(covarianzas_inicial)"
      ],
      "metadata": {
        "id": "Cslkd7qNhwIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación, podemos configurar el método con la inicialización obtenida y ejecutar el análisis de grupos. En la primera línea, estamos creando un objeto de tipo *observer* que nos va a permitir ver cómo varía el proceso de agrupamiento."
      ],
      "metadata": {
        "id": "gh5G8dUiiwrv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcLMDNVdF1GI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8381fcf-4aae-49da-bbc2-98717086ed5b"
      },
      "source": [
        "alg_ema = ema(datos_xy, num_grupos, medias_inicial, covarianzas_inicial);\n",
        "alg_ema.process()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyclustering.cluster.ema.ema at 0x7f29d2221dd0>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tras ejecutarlo, podemos extraer no solo los grupos, sino también las medias y covarianzas finales.\n",
        "\n"
      ],
      "metadata": {
        "id": "VlqpQYcGml6Z"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-rXALFLGmB_"
      },
      "source": [
        "grupos = alg_ema.get_clusters()\n",
        "print(grupos)\n",
        "medias_final = alg_ema.get_centers()\n",
        "print(medias_final)\n",
        "covarianzas_final = alg_ema.get_covariances()\n",
        "print(covarianzas_final)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por último, vamos a visualizar de forma dinámica cómo se han ido definiendo las distribuciones de probabilidad alrededor de los datos."
      ],
      "metadata": {
        "id": "s_no8AgrnykC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grafico = ema_visualizer.show_clusters(grupos, datos_xy, covarianzas_final, medias_final, display=False)\n",
        "grafico.show()"
      ],
      "metadata": {
        "id": "YUerUnTsn5yY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XqhcDCaKjxT"
      },
      "source": [
        "## 3. Ejecutar el algoritmo DBSCAN en ***scikit-learn***\n",
        "Vamos a utilizar el mismo conjunto de datos (Iris) pero ahora utilizaremos la implementación disponible en [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html). Para ello, primero importamos el algoritmo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SiIJJF_KoQP"
      },
      "source": [
        "from sklearn.cluster import DBSCAN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swjHi7oKC4jS"
      },
      "source": [
        "Vamos a utilizar la misma muestra de datos (datos_xy) y los mismos parámetros que en la ejecución con *PyClustering*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eWh2w1TDFxt"
      },
      "source": [
        "alg_dbscan_sklearn = DBSCAN(eps=0.25, min_samples=5)\n",
        "alg_dbscan_sklearn.fit(datos_xy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como DBSCAN asigna -1 a los datos etiquetados como ruido, debemos procesar por separado estos puntos. Por ejemplo, para conocer el número de grupos identificado podemos utilizar la siguiente línea de código:"
      ],
      "metadata": {
        "id": "f0GheBYqqeQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "etiquetas_dbscan = alg_dbscan_sklearn.labels_\n",
        "print(etiquetas_dbscan)\n",
        "num_grupos = len(set(etiquetas_dbscan)) - (1 if -1 in etiquetas_dbscan else 0)\n",
        "print(num_grupos)"
      ],
      "metadata": {
        "id": "-1NssYcCqpFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para conocer el número de puntos etiquetados como ruido, simplemente tenemos que contar el número de veces que aparece la etiqueta -1:"
      ],
      "metadata": {
        "id": "2ODyEwdZrMuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_outliers = list(etiquetas_dbscan).count(-1)\n",
        "print(num_outliers)"
      ],
      "metadata": {
        "id": "SjVBf3gQrTbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HyfN7tRKoqv"
      },
      "source": [
        "## 4. Ejecutar el algoritmo Expectation-Maximization en ***scikit-learn***\n",
        "En *scikit-learn* también podemos aplicar el método EM asociado al modelo de mezclas gaussianas. Puedes encontrar su documentación en el paquete [sklearn.mixture](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38KAbyiEFhRj"
      },
      "source": [
        "from sklearn.mixture import GaussianMixture"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En su configuración, podemos indicar el número de componentes gaussianas a estimar y la semilla para la inicialización aleatoria."
      ],
      "metadata": {
        "id": "mq_HDCGEtEJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alg_gmm_sklearn = GaussianMixture(n_components=3, random_state=0, init_params='random')\n",
        "alg_gmm_sklearn.fit(datos_xy)"
      ],
      "metadata": {
        "id": "woXlzTCzFDma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para evitar los inconvenientes de la inicialización aleatoria, podemos aplicar alguna de estas dos opciones:\n",
        "\n",
        "\n",
        "1.   Generar más de una inicialización aleatoria, quedándonos con la mejor. Para ello, se debe utilizar el parámetro *n_init*.\n",
        "2.   Utilizar el método k-means para inicializar, que de hecho es el valor por defecto del parámetro *init_params*.\n",
        "\n"
      ],
      "metadata": {
        "id": "6OeVNcbetZla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alg_gmm_sklearn = GaussianMixture(n_components=3, random_state=0, init_params='kmeans')\n",
        "alg_gmm_sklearn.fit(datos_xy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGgpyVC7FHjp",
        "outputId": "ff9dd836-7b93-4320-ac3a-50418337d683"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianMixture(n_components=3, random_state=0)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez ejecutado el método, podemos acceder a sus propiedades: pesos de cada componente del *mixture model*, medias, y covarianzas"
      ],
      "metadata": {
        "id": "cbwEE670FZdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pesos = alg_gmm_sklearn.weights_\n",
        "print(pesos)\n",
        "medias = alg_gmm_sklearn.means_\n",
        "print(medias)\n",
        "covarianzas = alg_gmm_sklearn.covariances_\n",
        "print(covarianzas)"
      ],
      "metadata": {
        "id": "CLineV7rupjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como es habitual en *sklearn*, podemos usar el método *predict* para predecir a qué grupo se asignaría una nueva muestra."
      ],
      "metadata": {
        "id": "SozRFsVsvQpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nuevo_dato = [[7,3.5]]\n",
        "prediccion = alg_gmm_sklearn.predict(nuevo_dato)\n",
        "print(prediccion)"
      ],
      "metadata": {
        "id": "2hhsIyquvb5P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
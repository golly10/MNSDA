{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Configurar_algoritmo_LOF.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Configurar algoritmo LOF\n",
        "\n",
        "En este ejemplo vamos a probar diferentes parámetros del algoritmo LOF para resolver un problema concreto. Muchas veces es necesario hacer un ajuste para cada problema debido a que presentan características muy diferentes.\n",
        "\n",
        "Para analizar el algoritmo vamos a evaluar las medidas: sensibilidad y especificación, para conseguir un compromiso entre ambas. Para su análisis se va a realizar una división del conjunto de datos, en datos de train y datos de test (como se han visto en todos los ejemplos estudiados) con la proporción (70% para training y 30% para test). Al ser un método estocástico, se deben realizar diferentes ejecuciones y trabajar con los valores medios, no obstante, en este pequeño estudio no llevaremos a cabo diferentes ejecuciones\n",
        "\n",
        "El algoritmo que se va a configurar es:\n",
        "        LOF: Local Outlier Factor\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4EUg9osITN2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 1. Instalar la librería PyOD"
      ],
      "metadata": {
        "id": "ChTHTlt6Gkn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyOD"
      ],
      "metadata": {
        "id": "RQejPHp4N-UC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 2. Importamos las librerías que necesitamos"
      ],
      "metadata": {
        "id": "kG6UQ5SRwiMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.io import loadmat\n",
        "\n",
        "from pyod.models.lof import LOF\n",
        "from pyod.utils.utility import standardizer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import os\n",
        "import sys"
      ],
      "metadata": {
        "id": "lf0WV8uqwpvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 3. Definimos la lista de datos que vamos a usar (3 en nuestro caso), previamente debemos haberlos cargado en google colaboration (como se ha explicado en el video), la semilla del generador de números aleatorios que se usará en los casos que sea necesario y las variables donde almacenaremos las métricas que se van a obtener para cada ejecución por cada algoritmo, se incluye el tiempo de cómputo."
      ],
      "metadata": {
        "id": "Qx_UJbsWxF-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos la lista de datos que vamos a usar \n",
        "mat_datasets = ['arrhythmia.mat',\n",
        "                 'cardio.mat',\n",
        "                 'glass.mat']\n",
        "\n",
        "\n",
        "# Definimos las variables que se van a utilizar para almacenar los resultados de \n",
        "# cada conjunto de datos, todas tendrán las mismas columnas y se hará una\n",
        "# tabla para comparar cada métrica de las estudiadas\n",
        "columnas_tabla = ['Algoritmo','Datos', '#Ejemplos', '#Dimensiones', 'Anomalías(%)','k',\n",
        "              'Se','Sp']\n",
        "\n"
      ],
      "metadata": {
        "id": "lR44wn3exkXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 4. Vamos a ejecutar el algoritmos con cada conjunto de datos y con diferente valor de k, primero preparamos los datos normalizándolos y dividiéndolos en train y test según la proporción (70% y 30%), a continuación aplicamos el algoritmo variando el parámetro que queremos estudiar.\n",
        "\n"
      ],
      "metadata": {
        "id": "n_YKSsK6yEk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos el conjunto de datos que vamos a evaluar\n",
        "mat_dataset = mat_datasets[2]\n",
        "result_tabla = pd.DataFrame(columns=columnas_tabla)\n",
        "dataset = loadmat(os.path.join('sample_data', mat_dataset))\n",
        "\n",
        "# Definimos la semilla que se va a utilizar, siendo un número aleatorio entre 1 y 50\n",
        "random_state = np.random.RandomState(50)\n",
        "\n",
        "# Separamos los atributos de la clase\n",
        "X = dataset['X']\n",
        "y = dataset['y'].ravel()\n",
        "\n",
        "# Obtenemos la fracción de anomalías que tiene el conjunto de datos\n",
        "# es una variable que necesitan los algoritmos\n",
        "anomalias_fraccion = np.count_nonzero(y) / len(y)\n",
        "anomalias_porcentaje = round(anomalias_fraccion * 100, ndigits=4)\n",
        "\n",
        "# Divimos los datos en train y test (70%, 30% respectivamente). También\n",
        "# se pueden usar otras formas de cross-validation \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
        "                                                        random_state=random_state)\n",
        "\n",
        "# Normalizamos los datos (usamos el métodos de PyOD)\n",
        "X_train_norm, X_test_norm = standardizer(X_train, X_test)\n",
        "\n",
        "   \n",
        "# Para cada configuración definido\n",
        "for k in range(5, 25, 1):\n",
        "      # Configuramos los algoritmos que vamos a usar\n",
        "       clf_name = 'LOF'\n",
        "       clf = LOF(contamination=anomalias_fraccion, n_neighbors = k)\n",
        "       \n",
        "       clf.fit(X_train_norm)\n",
        "       \n",
        "       # Obtenemos los resultados para los datos de train y test\n",
        "       y_train_pred = clf.labels_ \n",
        "       y_test_pred = clf.predict(X_test_norm)  \n",
        "       \n",
        "       #Calculamos sensibilidad y especificidad  desde la matriz de confusión\n",
        "       # para los datos de  test\n",
        "       cm = confusion_matrix(y_test,y_test_pred)\n",
        "       total=sum(sum(cm))\n",
        "       sensitivity = cm[1,1]/(cm[1,0]+cm[1,1])\n",
        "       specificity = cm[0,0]/(cm[0,0]+cm[0,1])\n",
        "       \n",
        "       se = round(sensitivity, ndigits=4)\n",
        "       sp = round(specificity, ndigits=4)\n",
        "       \n",
        "       #print('{clf_name}, k:{k} Se:{se}, Sp:{sp}'.format(clf_name=clf_name, k=k, se=se, sp=sp))\n",
        "       \n",
        "       # Definimos las listas para almacenar los resultados\n",
        "       result_list = [clf_name,mat_dataset[:-4], X.shape[0], X.shape[1], anomalias_porcentaje]\n",
        "       \n",
        "       result_list.append(k)\n",
        "       result_list.append(se)\n",
        "       result_list.append(sp)\n",
        "       \n",
        "       # Una vez finalizadas las ejecuciones de todos los métodos,\n",
        "       # pasamos la información a tabla\n",
        "       temp_tabla = pd.DataFrame(result_list).transpose()\n",
        "       temp_tabla.columns = columnas_tabla\n",
        "       result_tabla = pd.concat([result_tabla, temp_tabla], axis=0)\n",
        "\n",
        "print('Resultado LOF')\n",
        "result_tabla\n"
      ],
      "metadata": {
        "id": "NKCvOBCiTVJy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
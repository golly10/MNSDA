{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Comparar_modelos_anomalias.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Comparando modelos de detección de anomalías\n",
        "\n",
        "En este ejemplo vamos a diseñar un experimento para comparar todos los modelos basados en vecinos, basados en agrupamiento y HBOs que se han visto en este curso para resolver tres problemas de detección de anomalías del mundo real usando la librería PyOD.\n",
        "\n",
        "Vamos a usar datasets de la ODDs (Outlier Detection DataSets (ODDS)) http://odds.cs.stonybrook.edu. Que van a ser: arrythmia, cardio y glass, que se encuentran comentados en el documento que tenéis disponible.\n",
        " \n",
        "\n",
        "Las medidas que se van a analizar son sensibilidad, especificación, precisión y curva ROC. Para su análisis se va a realizar una división del conjunto de datos, en datos de train y datos de test (como se han visto en todos los ejemplos estudiados) con la proporción (70% para training y 30% para test). Además, como son métodos estocásticos se van a realizar 10 ejecuciones diferentes en cada uno de ellos.\n",
        "\n",
        "Los algoritmos que se encuentran en la librería PyOD son:\n",
        "\n",
        "        LOF: Local Outlier Factor\n",
        "        COF: Connectiviy Outlier Factor\n",
        "        CBLOF: Clustering-Based Local Outlier Factor\n",
        "        kNN: k Nearest Neighbors (use the distance to the kth nearest neighbor as the outlier score)\n",
        "        HBOS: Histogram-based Outlier Score\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4EUg9osITN2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 1. Instalar la librería PyOD"
      ],
      "metadata": {
        "id": "ChTHTlt6Gkn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyOD"
      ],
      "metadata": {
        "id": "RQejPHp4N-UC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 2. Importamos las librerías que necesitamos"
      ],
      "metadata": {
        "id": "kG6UQ5SRwiMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.io import loadmat\n",
        "\n",
        "\n",
        "from pyod.models.cblof import CBLOF\n",
        "from pyod.models.hbos import HBOS\n",
        "from pyod.models.knn import KNN\n",
        "from pyod.models.lof import LOF\n",
        "from pyod.models.cof import COF\n",
        "\n",
        "\n",
        "from pyod.utils.utility import standardizer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from time import time"
      ],
      "metadata": {
        "id": "lf0WV8uqwpvN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 3. Definimos la lista de datos que vamos a usar (3 en nuestro caso), previamente debemos haberlos cargado en google colaboration (como se ha explicado en el video), la semilla del generador de números aleatorios que se usará en los casos que sea necesario y las variables donde almacenaremos las métricas que se van a obtener para cada ejecución por cada algoritmo, se incluye el tiempo de cómputo."
      ],
      "metadata": {
        "id": "Qx_UJbsWxF-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos la lista de datos que vamos a usar \n",
        "mat_datasets = ['arrhythmia.mat',\n",
        "                 'cardio.mat',\n",
        "                 'glass.mat']\n",
        "\n",
        "\n",
        "# Definimos las variables que se van a utilizar para almacenar los resultados de \n",
        "# cada conjunto de datos, todas tendrán las mismas columnas y se hará una\n",
        "# tabla para comparar cada métrica de las estudiadas\n",
        "columnas_tabla = ['Datos', '#Ejemplos', '#Dimensiones', 'Anomalías(%)',\n",
        "              'HBOS', 'CBLOF', 'KNN', 'LOF', 'COF']\n",
        "\n",
        "\n",
        "#Configuramos los parámetros de cada algoritmo para cada dataset\n",
        "k_lof = [80,250,5]\n",
        "k_cof = [5,250,5]\n",
        "k_knn = [10,40,5]\n",
        "cluster_cblof = [6,5,50]\n",
        "bins_hbos = [13,3,13]\n",
        "\n",
        "#Definimos las variables\n",
        "se_tabla = pd.DataFrame(columns=columnas_tabla)\n",
        "sp_tabla = pd.DataFrame(columns=columnas_tabla)\n",
        "p_tabla = pd.DataFrame(columns=columnas_tabla)\n",
        "roc_tabla = pd.DataFrame(columns=columnas_tabla)\n",
        "time_tabla = pd.DataFrame(columns=columnas_tabla)"
      ],
      "metadata": {
        "id": "lR44wn3exkXq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 4. Vamos a ejecutar cada uno de los algoritmos con cada conjunto de datos, primero preparamos los datos normalizándolos y dividiéndolos en train y test según la proporción (70% y 30%), a continuación aplicamos los algoritmos con sus parámetros optimizados y almacenamos en las tablas los resultados obtenidos para cada una de las métricas consideradas."
      ],
      "metadata": {
        "id": "n_YKSsK6yEk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos la semilla que se va a utilizar, siendo un número aleatorio entre 1 y 50\n",
        "random_state = np.random.RandomState(42)\n",
        "\n",
        "#usamos los parámetros ajustados\n",
        "parameter = -1\n",
        "\n",
        "# Para cada dataset considerado en el paso anterior\n",
        "for mat_dataset in mat_datasets:\n",
        "\n",
        "    #Utilizamos los parámetros del dataset\n",
        "    parameter = parameter + 1\n",
        "    \n",
        "    # Cargamos el conjunto de datos\n",
        "    dataset = loadmat(os.path.join('sample_data', mat_dataset))\n",
        "\n",
        "    # Separamos los atributos de la clase\n",
        "    X = dataset['X']\n",
        "    y = dataset['y'].ravel()\n",
        "\n",
        "    # Obtenemos la fracción de anomalías que tiene el conjunto de datos\n",
        "    # es una variable que necesitan los algoritmos\n",
        "    anomalias_fraccion = np.count_nonzero(y) / len(y)\n",
        "    anomalias_porcentaje = round(anomalias_fraccion * 100, ndigits=4)\n",
        "\n",
        "    # Definimos las listas para almacenar los resultados\n",
        "    se_list = [mat_dataset[:-4], X.shape[0], X.shape[1], anomalias_porcentaje]\n",
        "    sp_list =[mat_dataset[:-4], X.shape[0], X.shape[1], anomalias_porcentaje]\n",
        "    p_list = [mat_dataset[:-4], X.shape[0], X.shape[1], anomalias_porcentaje]\n",
        "    roc_list = [mat_dataset[:-4], X.shape[0], X.shape[1], anomalias_porcentaje]\n",
        "    time_list = [mat_dataset[:-4], X.shape[0], X.shape[1], anomalias_porcentaje]\n",
        "\n",
        "    # Divimos los datos en train y test (70%, 30% respectivamente). También\n",
        "    # se pueden usar otras formas de cross-validation \n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
        "                                                        random_state=random_state)\n",
        "\n",
        "    # Normalizamos los datos (usamos el métodos de PyOD)\n",
        "    X_train_norm, X_test_norm = standardizer(X_train, X_test)\n",
        "\n",
        "\n",
        "    # Configuramos los algoritmos que vamos a usar\n",
        "    classificadores = {'Cluster-based Local Outlier Factor': CBLOF(\n",
        "            contamination=anomalias_fraccion, check_estimator=False,\n",
        "            random_state=random_state, n_clusters = cluster_cblof[parameter]),\n",
        "        'Histogram-base Outlier Detection (HBOS)': HBOS(\n",
        "            contamination=anomalias_fraccion, n_bins=bins_hbos[parameter]),\n",
        "        'K Nearest Neighbors (KNN)': KNN(contamination=anomalias_fraccion,\n",
        "            n_neighbors = k_knn[parameter]),\n",
        "        'Local Outlier Factor (LOF)': LOF(\n",
        "            contamination=anomalias_fraccion, n_neighbors = k_lof[parameter]),\n",
        "        'Connection Outlier Factor (COF)': COF(\n",
        "            contamination=anomalias_fraccion, n_neighbors = k_cof[parameter]),\n",
        "      }\n",
        "\n",
        "    # Para cada clasificador definido\n",
        "    for clf_name, clf in classificadores.items():\n",
        "        # Inicializamos el tiempo\n",
        "        t0 = time()\n",
        "        # Entrenamos el modelos con los datos normalizados\n",
        "        clf.fit(X_train_norm)\n",
        "\n",
        "        # Obtenemos los resultados para los datos de train y test\n",
        "        y_train_pred = clf.labels_ \n",
        "        y_test_pred = clf.predict(X_test_norm)  \n",
        "\n",
        "       # test_scores = clf.decision_function(X_test_norm)\n",
        "        # Una vez tenemos los resultados finalizamos el tiempo\n",
        "        t1 = time()\n",
        "        duracion = round(t1 - t0, ndigits=4)\n",
        "        time_list.append(duracion)\n",
        "\n",
        "\n",
        "        #Calculamos sensibilidad, especificidad y precision desde la matriz de confusión\n",
        "        # para los datos de  test\n",
        "        cm = confusion_matrix(y_test,y_test_pred)\n",
        "        total=sum(sum(cm))\n",
        "        sensitivity = cm[1,1]/(cm[1,0]+cm[1,1])\n",
        "        specificity = cm[0,0]/(cm[0,0]+cm[0,1])\n",
        "        precision = cm[1,1]/(cm[1,1]+cm[0,1])\n",
        "\n",
        "        se = round(sensitivity, ndigits=4)\n",
        "        sp = round(specificity, ndigits=4)\n",
        "        p = round(precision, ndigits=4)\n",
        "        roc = round(roc_auc_score(y_test, y_test_pred), ndigits=4)\n",
        "    \n",
        "        print('{clf_name} Se:{se}, Sp:{sp}, precisión:{p}, ROC:{roc}, '\n",
        "              'tiempo ejecución: {duracion}'.format(clf_name=clf_name, se=se, sp=sp, roc=roc, p=p, duracion=duracion))\n",
        "\n",
        "        se_list.append(se)\n",
        "        sp_list.append(sp)\n",
        "        p_list.append(p)\n",
        "        roc_list.append(roc)\n",
        "\n",
        "\n",
        "    # Una vez finalizadas las ejecuciones de todos los métodos,\n",
        "    # pasamos la información a tabla\n",
        "    temp_tabla = pd.DataFrame(time_list).transpose()\n",
        "    temp_tabla.columns = columnas_tabla\n",
        "    time_tabla = pd.concat([time_tabla, temp_tabla], axis=0)\n",
        "\n",
        "    temp_tabla = pd.DataFrame(se_list).transpose()\n",
        "    temp_tabla.columns = columnas_tabla\n",
        "    se_tabla = pd.concat([se_tabla, temp_tabla], axis=0)\n",
        "\n",
        "    temp_tabla = pd.DataFrame(sp_list).transpose()\n",
        "    temp_tabla.columns = columnas_tabla\n",
        "    sp_tabla = pd.concat([sp_tabla, temp_tabla], axis=0)\n",
        "\n",
        "    temp_tabla = pd.DataFrame(roc_list).transpose()\n",
        "    temp_tabla.columns = columnas_tabla\n",
        "    roc_tabla = pd.concat([roc_tabla, temp_tabla], axis=0)\n",
        "\n",
        "    temp_tabla = pd.DataFrame(p_list).transpose()\n",
        "    temp_tabla.columns = columnas_tabla\n",
        "    p_tabla = pd.concat([p_tabla, temp_tabla], axis=0)"
      ],
      "metadata": {
        "id": "NKCvOBCiTVJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 5. Mostramos la tablas de resultados, para analizarlas"
      ],
      "metadata": {
        "id": "FAFTglo_4l4J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mostramos el tiempo de cómputo"
      ],
      "metadata": {
        "id": "3HhNxioG4yeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Tiempo de cómputo')\n",
        "time_tabla"
      ],
      "metadata": {
        "id": "B_pk0mqUTbHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mostramos la sensibilidad\n"
      ],
      "metadata": {
        "id": "szf92e9JTi0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Sensibilidad')\n",
        "se_tabla"
      ],
      "metadata": {
        "id": "rsggPUpy45GO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mostramos la especificidad"
      ],
      "metadata": {
        "id": "BofLaxTX4894"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Especificidad')\n",
        "sp_tabla"
      ],
      "metadata": {
        "id": "oacQpndk5ApW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mostramos la precisión"
      ],
      "metadata": {
        "id": "XCXMJwns5DwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Precisión')\n",
        "p_tabla"
      ],
      "metadata": {
        "id": "miK3rvzc5FXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mostramos el área bajo la curva roc"
      ],
      "metadata": {
        "id": "pZLd4Mi55IMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('ROC')\n",
        "roc_tabla"
      ],
      "metadata": {
        "id": "XH8UqhonTems"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BZydxQJz5Pd2"
      }
    }
  ]
}
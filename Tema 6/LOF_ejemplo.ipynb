{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LOF_ejemplo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LOF para detección de anomalías\n",
        "\n",
        "Aplicamos el método LOF basado en vecinos más cercanos para detección de anomalías.\n",
        "\n",
        "Utilizamos la librería PyOD que es una biblioteca de Python para detectar anomalías en datos multivariados. La biblioteca fue desarrollada por Yue Zhao."
      ],
      "metadata": {
        "id": "86bXDH5L7pa_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 1. Instalar la librería PyOD"
      ],
      "metadata": {
        "id": "kXHDAduP8jbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyOD"
      ],
      "metadata": {
        "id": "LiuBh5eW4IOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 2. Importamos las librerías que necesitamos"
      ],
      "metadata": {
        "id": "yMC_r9pk8o3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pyod.models.lof import LOF \n",
        "from pyod.utils.data import generate_data\n",
        "from pyod.utils.example import visualize\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "metadata": {
        "id": "j03i8nGs4Daq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 3. Generamos datos sintéticos. En este caso, vamos a trabajar con datos de entrenamiento y datos de test, tal y como se ha explicado en clase. Para tener un conjunto considerable de datos de forma rápida, vamos a generarlos utilizando el método generate_data que nos permite definir el número de instancias, las dimensiones y el porcentaje de outliers que queremos.\n",
        "\n",
        "Rercordamos que aunque el algoritmo no usará el atributo que determina si es una anomalía o no para realizar su procesamiento, las tenemos disponibles con la finalidad de comparar los resultados."
      ],
      "metadata": {
        "id": "jDF7Mmye88T2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QACrKl7v4CPV"
      },
      "outputs": [],
      "source": [
        "# Porcentaje de outliers que vamos a considerar\n",
        "contamination = 0.1  \n",
        "\n",
        "#Número de ejemplos de entrenamiento y test que vamos a utilizar\n",
        "n_train = 200  \n",
        "n_test = 100  \n",
        "\n",
        "# Generamos los datos, considerando 2 dimensiones y las características anteriores\n",
        "# Utilizamos como generador de datos aleatorios (random_state) la semilla 12, \n",
        "# para que en todas las pruebas obtengamos los mismos datos.\n",
        "X_train, X_test, y_train, y_test = generate_data(n_train=n_train, n_test=n_test, n_features=2, behaviour='new', contamination=contamination,random_state=12)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 4. Visualizamos los datos que hemos generado para train (entrenamiento) y test (prueba), lo representamos en diferente color los puntos que son anomalías de los que no.\n"
      ],
      "metadata": {
        "id": "8ZQAnD6S-8Sm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creamos una nueva variable DataFrame para que incorpore en los datos de entrenamiento\n",
        "# el atributo la etiqueta de anomalía o no.\n",
        "# Lo usamos solamente para la representación, los datos que usará el modelo sigue\n",
        "#sin tener esa información\n",
        "XY_train = pd.DataFrame({\"Variable 1\": X_train[:,0], \"Variable 2\": X_train[:,1], \"Anomalía\": y_train})\n",
        "\n",
        "#Lo representamos gráficamente poniendo el color en función de si es anómalo o no\n",
        "plt.scatter(XY_train['Variable 1'], XY_train['Variable 2'], c=XY_train['Anomalía'])\n",
        "plt.xlabel('Variable 1')\n",
        "plt.ylabel('Variable 2')\n",
        "plt.title(\"Distribución de los datos training\")\n",
        "plt.show()\n",
        "\n",
        "#Creamos una nueva variable DataFrame para que incorpore en los datos de test \n",
        "# el atributo la etiqueta de anomalía o no solamente para la representación. \n",
        "# El conjunto de test que usaremos en el modelo sigue sin tener esa información\n",
        "XY_test = pd.DataFrame({\"Variable 1\": X_test[:,0], \"Variable 2\": X_test[:,1], \"Anomalía\": y_test})\n",
        "\n",
        "#Lo representamos gráficamente poniendo el color en función de si es anómalo o no\n",
        "plt.scatter(XY_test['Variable 1'], XY_test['Variable 2'], c=XY_test['Anomalía'])\n",
        "plt.xlabel('Variable 1')\n",
        "plt.ylabel('Variable 2')\n",
        "plt.title(\"Distribución de los datos test\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oTpwSCh8_AJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 4. Entrenamos el modelo LOF utilizando la librería pyOD\n",
        "\n",
        "Parámetros:\n",
        "\n",
        "    n_neighbors : integer, (defecto = 20) - El número de vecinos que se van a utilizar.\n",
        "    \n",
        "    contamination: float (0., 0.5), (defecto = 0.1)) – La cantidad de contaminación del conjunto de datos. La proporción de valores atípicos en el conjunto de datos. Se utiliza cuando para definir el umbral en la función de decisión.\n",
        "    \n",
        "    metric : string, (defecto = 'minkowski') - la métrica usada para calcular las distancias. Se pueden usar las métricas disponibles en scikit-learn, por ejemplo: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2','manhattan']. También desde: scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev', 'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski', 'mahalanobis', 'matching', 'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule']\n",
        "    \n",
        "    p : integer, (defecto = 2) - parámetro que se usa para la distancia de Minkowski. Cuando p = 1, es equivalente a usar la distancia de manhattan (l1) y si p=2, es equivalente a la distancia euclídea (l2).\n"
      ],
      "metadata": {
        "id": "I_N5DO7wIeAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Indicamos que utilizamos el método LOF y lo entrenamos usando el método fit\n",
        "clf_name = 'LOF'\n",
        "clf = LOF(n_neighbors=20, metric='minkowski', contamination = 0.1, novelty = True)\n",
        "clf.fit(X_train)"
      ],
      "metadata": {
        "id": "BSdX3jQe9wHT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "184d51eb-c3a0-43f0-a88e-9e96aaeb5a57"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LOF(algorithm='auto', contamination=0.1, leaf_size=30, metric='manhattan',\n",
              "  metric_params=None, n_jobs=1, n_neighbors=20, novelty=True, p=2)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 5. Obtenemos los valores predichos por el modelo para los datos de train y de test. Obtenemos tanto la etiqueta binaria que le asigna este método a cada instancia, como las puntuaciones que se le asignan. De nuevo, usamos los métodos dados por la librería PyOD."
      ],
      "metadata": {
        "id": "6sD7VIjyIytt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtenemos las etiquetas y los scores para cada instancia de train.\n",
        "# Las etiquetas asignan un 0 a los datos normales y un 1 a las anomalías\n",
        "# Los scores son puntuaciones que usa el método para determinar si son o no anomalías en funcion de un umbral.\n",
        "y_train_pred = clf.labels_  \n",
        "y_train_scores = clf.decision_scores_  \n",
        "\n",
        "# Obtenemos las etiquetas y los scores para cada instancia de test.\n",
        "# Las etiquetas asignan un 0 a los datos normales y un 1 a las anomalías\n",
        "# Los scores son puntuaciones que usa el método para determinar si son o no anomalías en funcion de un umbral.\n",
        "y_test_pred = clf.predict(X_test)  \n",
        "y_test_scores = clf.decision_function(X_test) \n",
        "\n",
        "\n",
        "#Mostramos las etiquetas asignadas a cada instancia tanto reales como predichas \n",
        "print('Etiquetas reales datos train: ', y_train)\n",
        "print('Etiquetas reales datos test: ', y_test)\n",
        "\n",
        "print('Etiquetas predichas datos train: ', y_train_pred)\n",
        "print('Etiquetas predichas datos test: ', y_test_pred)"
      ],
      "metadata": {
        "id": "mnlidXyhImrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 6. Visualizamos los resultados para ver si el método ha reconocido adecuadamente todas las anomalías.\n"
      ],
      "metadata": {
        "id": "LY6i7yuAJvBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizamos los resultados de forma sencilla utilizando el método visualize\n",
        "# que tiene disponible PyOD y en el que solamente tenemos que pasarle los datos y\n",
        "# los valores predichos.\n",
        "\n",
        "visualize(clf_name, X_train, y_train, X_test, y_test, y_train_pred, y_test_pred, show_figure=True, save_figure=False)"
      ],
      "metadata": {
        "id": "E8fawqBh4Gg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comentario visualización:** se puede apreciar que tanto en los datos de entrenamiento como en los de test, tenemos fallos tanto a la hora de detectar las anomalías como los datos normales."
      ],
      "metadata": {
        "id": "vN6evpOOKqLX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 7. Calculamos las medidas que hemos visto para evaluar el rendimiento del método, partiendo de la matriz de confusión y también usando directamente la librería skearn.metrics"
      ],
      "metadata": {
        "id": "BRfdvm18z6cP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mostramos la matriz de confusión para datos de train\n",
        "ConfusionMatrixDisplay.from_predictions(y_train, y_train_pred)\n",
        "plt.title(\"Datos de train\")\n",
        "plt.show()\n",
        "\n",
        "#Mostramos la matriz de confusión para datos de test\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_test_pred)\n",
        "plt.title(\"Datos de test\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EGVJMc920FlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación vamos a mostrar la curva ROC-AUC, sensibilidad, especificidad y precisión."
      ],
      "metadata": {
        "id": "m0kygXaK0OJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculamos las medidas para train\n",
        "print('Medidas para train')\n",
        "print('------------------')\n",
        "#Calculamos la matriz de confusión\n",
        "cm = confusion_matrix(y_train,y_train_pred)\n",
        "print('Matriz de confusión : \\n', cm)\n",
        "total=sum(sum(cm))\n",
        "\n",
        "#Calculamos sensibilidad y especificidad desde la matriz de confusión\n",
        "sensitivity = cm[1,1]/(cm[1,0]+cm[1,1])\n",
        "print('Sensibilidad : ', sensitivity )\n",
        "specificity = cm[0,0]/(cm[0,0]+cm[0,1])\n",
        "print('Especificidad: ', specificity)\n",
        "precision = cm[1,1]/(cm[1,1]+cm[0,1])\n",
        "print('Precisión: ', precision)\n",
        "\n",
        "#Calculamos las otras métricas desde la librería\n",
        "print('ROC-AUC: ', roc_auc_score(y_train, y_train_pred, average=None))\n",
        "\n",
        "\n",
        "#Calculamos las medidas para test\n",
        "print('Medidas para test')\n",
        "print('------------------')\n",
        "#Calculamos la matriz de confusión\n",
        "cm = confusion_matrix(y_test,y_test_pred)\n",
        "print('Matriz de confusión : \\n', cm)\n",
        "total=sum(sum(cm))\n",
        "\n",
        "#Calculamos sensibilidad y especificidad desde la matriz de confusión\n",
        "sensitivity = cm[1,1]/(cm[1,0]+cm[1,1])\n",
        "print('Sensibilidad : ', sensitivity )\n",
        "specificity = cm[0,0]/(cm[0,0]+cm[0,1])\n",
        "print('Especificidad: ', specificity)\n",
        "precision = cm[1,1]/(cm[1,1]+cm[0,1])\n",
        "print('Precisión: ', precision)\n",
        "\n",
        "#Calculamos las otras métricas desde la librería\n",
        "print('ROC-AUC: ', roc_auc_score(y_test, y_test_pred, average=None))\n"
      ],
      "metadata": {
        "id": "bDh1uL6C0TUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comentario métricas:** se puede ver que los datos de test y train obtienen valores similares. En ambos conjuntos de datos la especifididad es más alta que la sensibilidad, lo que indica que clasifica mejor la clase normal que la anómala. La precisión entorno a un 75%, indica que de todo lo que dice que es anómalo, hay ejemplos normales. Es decir, se detectan como anomalías datos que son normales. En aplicaciones críticas, estos fallos son preferibles a los fallos donde anomalías reales son identificadas como normales, pero este algoritmo también comete ese tipo de fallos, poniéndose de manifiesto con el valor de la sensibilidad entorno a un 75% también.\n",
        "\n",
        "  El área de la curva ROC tenemos unos valores alrededor del 85%, indicándonos que esa sería la probabilidad de clasificar correctamente los datos normales y las anomalías. Te animo a que consideres modificar los parámetros de LOF de número de vecinos a 10 por ejemplo y a 30, ¿qué nos interesa considerar más o menos vecinos para mejorar los resultados?. Podemos cambiar también la distancia, ¿qué resultados se obtienen si ponemos la de manhattan con 20 vecinos?, ¿qué parámetro ha resultado más relevante?.\n",
        "  "
      ],
      "metadata": {
        "id": "ipYINZRQ0ZmY"
      }
    }
  ]
}